declare const _default: "\nimport sys\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Set encoding\nsys.stdout.reconfigure(encoding='utf-8')\n\ndef main(*args):\n  model_name = args[0]\n  src_lang = args[1]\n  tgt_lang = args[2]\n  text = args[3]\n\n  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n  tokenizer = AutoTokenizer.from_pretrained(\n    model_name, \n    # Default \"/USER/.cache/huggingface/hub\"\n    # cache_dir=\"./.nllb-200-models\", \n    src_lang=src_lang\n  )\n\n  # https://github.com/huggingface/transformers/issues/2704\n  model = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name, \n    # cache_dir=\"./.nllb-200-models\",\n  ).to(device)\n\n  inputs = tokenizer(\n    text=text, return_tensors=\"pt\"\n  ).to(device)\n  \n  # max_length <= 512\n  # Deprecated: tokenizer.lang_code_to_id\n  translated_tokens = model.generate(\n    **inputs, forced_bos_token_id=tokenizer.encode(tgt_lang)[1], max_length=384,\n  )\n\n  output = tokenizer.batch_decode(\n    translated_tokens, skip_special_tokens=True,\n  )[0]\n\n  sys.stdout.write(output)\n  sys.exit(0)\n\nif __name__ == '__main__':\n  main(*sys.argv[1:])\n";
export default _default;
//# sourceMappingURL=nllb-200.d.ts.map